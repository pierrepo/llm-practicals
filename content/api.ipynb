{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce6faf5a-c6b9-46d4-ae6a-af75bb13940b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-22T19:12:31.801910Z",
     "iopub.status.busy": "2024-10-22T19:12:31.801643Z",
     "iopub.status.idle": "2024-10-22T19:12:31.809340Z",
     "shell.execute_reply": "2024-10-22T19:12:31.808256Z",
     "shell.execute_reply.started": "2024-10-22T19:12:31.801886Z"
    }
   },
   "source": [
    "# Playing with LLM APIs\n",
    "\n",
    "In this notebook, we will use large language models (LLMs) through an API. Most LLM providers offer an API to use their LLMs programmatically (for instance, with Python).\n",
    "\n",
    "[Groq](https://groq.com/) freely provides (as of October 2024) access to some open LLMs, like Llama from Meta or Mixtral from Mistral AI.\n",
    "\n",
    "## Get credentials and API key\n",
    "\n",
    "- Create a free account on Groq: <https://console.groq.com/login>\n",
    "\n",
    "  You can use your GitHub or Google account for authenication.\n",
    "- Then go on [Groq's API keys](https://console.groq.com/keys) page\n",
    "- Click the black button  \"Create API Key\"\n",
    "- Chose a name for your key.\n",
    "- Copy your API key and paste it somewhere. For security reason, your API key won't be shown again. Your API key should look like: `gsk_W1Jz47u5ieJs28D8m...`\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56268464-22c3-4d87-a668-0806599932cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-22T20:56:16.092910Z",
     "iopub.status.busy": "2024-10-22T20:56:16.092542Z",
     "iopub.status.idle": "2024-10-22T20:56:16.110724Z",
     "shell.execute_reply": "2024-10-22T20:56:16.109835Z",
     "shell.execute_reply.started": "2024-10-22T20:56:16.092872Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "From university computers, use the Conda environment `ppoulain-llm-24`:\n",
       "\n",
       "```bash\n",
       "$ conda activate ppoulain-llm-24\n",
       "```\n",
       "\n",
       "You can also try to create this environement on your own computer.\n",
       "\n",
       "Either with [Miniconda](https://docs.anaconda.com/miniconda/):\n",
       "\n",
       "```bash\n",
       "$ mkdir -p llm-practicals\n",
       "$ cd llm-practicals\n",
       "$ curl https://raw.githubusercontent.com/pierrepo/llm-practicals/main/content/practical-env.yml --output practical-env.yml\n",
       "# or wget https://raw.githubusercontent.com/pierrepo/llm-practicals/main/content/practical-env.yml\n",
       "$ conda env create -f practical-env.yml\n",
       "$ conda activate ppoulain-llm-24\n",
       "$ jupyter lab\n",
       "```\n",
       "\n",
       "or with [Pixi](https://pixi.sh):\n",
       "\n",
       "```bash\n",
       "$ mkdir -p llm-practicals\n",
       "$ cd llm-practicals\n",
       "$ curl https://raw.githubusercontent.com/pierrepo/llm-practicals/main/content/practical-env.yml --output practical-env.yml\n",
       "# or wget https://raw.githubusercontent.com/pierrepo/llm-practicals/main/content/practical-env.yml\n",
       "$ pixi init --import practical-env.yml\n",
       "$ pixi run jupyter lab\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display; display(Markdown(\"env_instructions.md\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda7e69a-a7f1-4b49-814f-72da5f3b0c28",
   "metadata": {},
   "source": [
    "## Use Groq's API\n",
    "\n",
    "Declare your API key with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4669842d-a75a-45d4-b725-500e5f175b88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GROQ_API_KEY\"] = \"YOUR-API-KEY-HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4ba0bd-faa7-448c-bc82-44982264463b",
   "metadata": {},
   "source": [
    "Replace `YOUR-API-KEY-HERE` with your real Groq API key.\n",
    "\n",
    "Then, you can send your first prompt to a Llama LLM model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39b7131-fa7a-4010-8ed8-2068e3732b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "\n",
    "client = Groq(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    ")\n",
    "\n",
    "def chat(question, model):\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[ {\"role\": \"user\", \"content\": question} ],\n",
    "        model=model,\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content\n",
    "\n",
    "\n",
    "print(\n",
    "    \"=\"*50,\n",
    "    chat(\n",
    "        \"Explain in a short text what is bioinformatics to a middle school student\",\n",
    "        \"llama3-8b-8192\"\n",
    "    ),\n",
    "    \"=\"*50,\n",
    "    sep=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da16e9e-74eb-4d5e-b34b-8efb412320ab",
   "metadata": {},
   "source": [
    "Look at [available LLM models in Groq](https://console.groq.com/docs/models) and try with two different models. \n",
    "\n",
    "```{warning}\n",
    "Be sure to select a LLM that works with chat. For instance, `distil-whisper-large-v3-en` is not a chat model.\n",
    "```\n",
    "\n",
    "Here is another example with `llama-3.2-3b-preview`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b5c09c-2689-42fc-a14a-36c388a21568",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"=\"*50,\n",
    "    chat(\n",
    "        \"Explain in a short text what is bioinformatics to a middle school student\",\n",
    "        \"llama-3.2-3b-preview\"\n",
    "    ),\n",
    "    \"=\"*50,\n",
    "    sep=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36eee39d-ab5d-49cd-89e6-5d7b63d13718",
   "metadata": {},
   "source": [
    "## Are LLMs good at math?\n",
    "\n",
    "> Spoiler: no!\n",
    "\n",
    "In the following, we will assess the performance of a LLM in simple math calculations.\n",
    "\n",
    "### Find the correct prompt\n",
    "\n",
    "Design a prompt that forces the LLM to output only the result of the product of two integers. Test your prompt with several time with different integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6c4d81-1899-469d-86c3-6a4d33538000",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat(\n",
    "    \"YOUR-CLEVER-PROMPT-HERE\",\n",
    "    \"llama-3.2-3b-preview\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0778ea66-cab8-4fbf-b133-6e9e2a79c397",
   "metadata": {},
   "source": [
    "## LLM vs Python\n",
    "\n",
    "In the following code, we ask several times a LLM the result of the product of two integers and compare the output to the correct answer provided by Python.\n",
    "\n",
    "Take some time to understand how the code works.\n",
    "\n",
    "Run the code, play with it and make some improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8aeda1-9893-491b-9dca-4bab52ff0ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "correct_answers = 0\n",
    "total_attempts = 20\n",
    "\n",
    "for _ in range(total_attempts):\n",
    "    number_1 = randint(10, 100)\n",
    "    number_2 = randint(10, 100)\n",
    "    llm_result = chat(\n",
    "        f\"What is the result of {number_1} times {number_2}. Only reply with the result\",\n",
    "        \"llama3-8b-8192\"\n",
    "    )\n",
    "    # Remove extra characters from LLM output.\n",
    "    llm_result = llm_result.replace(\",\", \"\").replace(\".\", \"\")\n",
    "    # Get Python result.\n",
    "    python_result = number_1 * number_2\n",
    "    print(f\"{number_1} x {number_2} = {python_result} | {llm_result} \", end=\"\")\n",
    "    # Check result.\n",
    "    if llm_result == str(python_result):\n",
    "        correct_answers += 1\n",
    "        print(f\"OK\")\n",
    "    else:\n",
    "        print(f\"Wrong!\")\n",
    "\n",
    "print(f\"Success rate: {correct_answers/total_attempts:.0%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b329543-70f8-4f3a-92c6-4e421d490698",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
