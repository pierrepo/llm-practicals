{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffcc70a2-016f-442c-a129-4a3d0c90551c",
   "metadata": {},
   "source": [
    "# Tokenizer and embeddings with Python\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb32c14c-209f-466d-aaeb-b344e2b5b534",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-22T20:57:08.733038Z",
     "iopub.status.busy": "2024-10-22T20:57:08.732558Z",
     "iopub.status.idle": "2024-10-22T20:57:08.748457Z",
     "shell.execute_reply": "2024-10-22T20:57:08.747798Z",
     "shell.execute_reply.started": "2024-10-22T20:57:08.732995Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "From university computers, use the Conda environment `ppoulain-llm-24`:\n",
       "\n",
       "```bash\n",
       "$ conda activate ppoulain-llm-24\n",
       "```\n",
       "\n",
       "You can also try to create this environement on your own computer.\n",
       "\n",
       "Either with [Miniconda](https://docs.anaconda.com/miniconda/):\n",
       "\n",
       "```bash\n",
       "$ mkdir -p llm-practicals\n",
       "$ cd llm-practicals\n",
       "$ curl https://raw.githubusercontent.com/pierrepo/llm-practicals/main/content/practical-env.yml --output practical-env.yml\n",
       "# or wget https://raw.githubusercontent.com/pierrepo/llm-practicals/main/content/practical-env.yml\n",
       "$ conda env create -f practical-env.yml\n",
       "$ conda activate ppoulain-llm-24\n",
       "$ jupyter lab\n",
       "```\n",
       "\n",
       "or with [Pixi](https://pixi.sh):\n",
       "\n",
       "```bash\n",
       "$ mkdir -p llm-practicals\n",
       "$ cd llm-practicals\n",
       "$ curl https://raw.githubusercontent.com/pierrepo/llm-practicals/main/content/practical-env.yml --output practical-env.yml\n",
       "# or wget https://raw.githubusercontent.com/pierrepo/llm-practicals/main/content/practical-env.yml\n",
       "$ pixi init --import practical-env.yml\n",
       "$ pixi run jupyter lab\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display; display(Markdown(\"env_instructions.md\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abd09c6-d42e-4787-adfe-fe8852e7d688",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "\n",
    "[tiktoken](https://github.com/openai/tiktoken) is an open-source Python library developped by OpenAI to tokenize text. This library works fully locally and does not require any internet connection.\n",
    "\n",
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df298dbe-6cb8-4853-8440-6a2009579f68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-22T20:57:08.749360Z",
     "iopub.status.busy": "2024-10-22T20:57:08.749098Z",
     "iopub.status.idle": "2024-10-22T20:57:09.181497Z",
     "shell.execute_reply": "2024-10-22T20:57:09.180960Z",
     "shell.execute_reply.started": "2024-10-22T20:57:08.749333Z"
    }
   },
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39718f01-3867-439b-8fd3-887813422b6e",
   "metadata": {},
   "source": [
    "### First example\n",
    "\n",
    "Get tokens id from a simple sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7976f5a-2bef-488f-b235-18effce67ace",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-22T20:57:09.183082Z",
     "iopub.status.busy": "2024-10-22T20:57:09.182917Z",
     "iopub.status.idle": "2024-10-22T20:57:09.186800Z",
     "shell.execute_reply": "2024-10-22T20:57:09.186287Z",
     "shell.execute_reply.started": "2024-10-22T20:57:09.183067Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9906, 1917]\n"
     ]
    }
   ],
   "source": [
    "tokens = enc.encode(\"Hello world\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d835ff-141a-42de-a976-59c61d8ee611",
   "metadata": {},
   "source": [
    "Visualize tokens by separating them with `|`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c096433f-0213-4407-adc8-6411d408c78e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-22T20:57:09.187471Z",
     "iopub.status.busy": "2024-10-22T20:57:09.187299Z",
     "iopub.status.idle": "2024-10-22T20:57:09.239751Z",
     "shell.execute_reply": "2024-10-22T20:57:09.239296Z",
     "shell.execute_reply.started": "2024-10-22T20:57:09.187456Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello| world\n"
     ]
    }
   ],
   "source": [
    "print(\"|\".join([ enc.decode([tok]) for tok in tokens]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880f388c-1828-4265-931f-2a0184738e0f",
   "metadata": {},
   "source": [
    "The first token is `Hello` and the second is `  world` (with a space before `world`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcf7f08-e9b1-4a3b-abbc-5eb48b951219",
   "metadata": {},
   "source": [
    "### Hello bioinformatics\n",
    "\n",
    "Let's try with another sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7abb2d44-abfe-46cb-bc5f-6f93163549fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-22T20:57:09.240283Z",
     "iopub.status.busy": "2024-10-22T20:57:09.240158Z",
     "iopub.status.idle": "2024-10-22T20:57:09.244143Z",
     "shell.execute_reply": "2024-10-22T20:57:09.243730Z",
     "shell.execute_reply.started": "2024-10-22T20:57:09.240270Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9906, 17332, 98588]\n",
      "Hello| bio|informatics\n"
     ]
    }
   ],
   "source": [
    "tokens = enc.encode(\"Hello bioinformatics\")\n",
    "print(tokens)\n",
    "print(\"|\".join([ enc.decode([tok]) for tok in tokens]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bae090a-b87d-429b-85c6-6e7b95730f89",
   "metadata": {},
   "source": [
    "We have this time 3 tokens.\n",
    "\n",
    "Here is the same sentence in a different language:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00cea7b0-6fce-4152-883e-2d851c538b0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-22T20:57:09.244760Z",
     "iopub.status.busy": "2024-10-22T20:57:09.244633Z",
     "iopub.status.idle": "2024-10-22T20:57:09.248642Z",
     "shell.execute_reply": "2024-10-22T20:57:09.248220Z",
     "shell.execute_reply.started": "2024-10-22T20:57:09.244747Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17691, 332, 1208, 17332, 258, 2293, 2428]\n",
      "Sal|ut| la| bio|in|format|ique\n"
     ]
    }
   ],
   "source": [
    "tokens = enc.encode(\"Salut la bioinformatique\")\n",
    "print(tokens)\n",
    "print(\"|\".join([ enc.decode([tok]) for tok in tokens]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f3242e-2c45-4853-855a-b48d2bbf49af",
   "metadata": {},
   "source": [
    "The word `bioinformatics` is expressed in 2 tokens whereas its French equivalent (`bioinformatique`) is made of 4 tokens.\n",
    "\n",
    "Tokenizers are optimized for the English language. Equivalent sentences in other languages usually takes more tokens. This is an important difference considering that costs to use LLM APIs are usually per (million) tokens.\n",
    "\n",
    "### Explore by yourself\n",
    "\n",
    "Compare tokenization of other sentences or words in different languages (English, French, Italian, Russian, Chinese...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bfafc1-407f-436b-a2bd-ade0dc5a711b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023f8029-847f-4613-aecb-7bbc65d0f184",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f5279d-d7d6-4c37-b610-4860b66e6ead",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64aa1c5-8828-4450-bfe9-6343b33a0637",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96f7502c-e120-474f-93f3-06c26ea20f82",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f03955-92e8-4ec8-9520-193b4dfccabd",
   "metadata": {},
   "source": [
    "[sentence-transformers](https://github.com/UKPLab/sentence-transformers) is a Python library for computing sentence embeddings. This library works fully locally but requires an internet connection to download embedding models.\n",
    "\n",
    "The documentation is available [here](https://www.sbert.net/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a372898-b938-4d77-8be5-045db83f9ff1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-22T20:57:09.249301Z",
     "iopub.status.busy": "2024-10-22T20:57:09.249148Z",
     "iopub.status.idle": "2024-10-22T20:57:14.253775Z",
     "shell.execute_reply": "2024-10-22T20:57:14.253309Z",
     "shell.execute_reply.started": "2024-10-22T20:57:09.249285Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pierre/.local/share/miniconda3/envs/ppoulain-llm-24/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6f29fe-1d98-4b60-9e51-c93f60f97179",
   "metadata": {},
   "source": [
    "*The warning message in red is OK.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d08c06e-33c1-4e16-82c5-e716e250e896",
   "metadata": {},
   "source": [
    "### (Down)Load model\n",
    "\n",
    "We load the [all-mpnet-base-v2]((https://huggingface.co/sentence-transformers/all-mpnet-base-v2)) embedding model. It's a rather *small* embedding model (109M --109 millions-- parameters) hosted on [HuggingFace](https://huggingface.co/). It converts any input text into a vector of 768 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cdbd57d-beee-48d6-9a20-b811d43a0743",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-22T20:57:14.254561Z",
     "iopub.status.busy": "2024-10-22T20:57:14.254256Z",
     "iopub.status.idle": "2024-10-22T20:57:14.257353Z",
     "shell.execute_reply": "2024-10-22T20:57:14.256769Z",
     "shell.execute_reply.started": "2024-10-22T20:57:14.254544Z"
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"all-mpnet-base-v2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a590016-3e09-494f-b5de-cce016958829",
   "metadata": {},
   "source": [
    "The first time the model is called, it will be downloaded locally. This can take some time and disk space (about 419 MB). By default, models are stored in `$HOME/.cache/huggingface/`. If you're using university computers, to avoid overloading your HOME directory and also the NFS server that supports it, we specify where models are stored locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f29f1ad0-9d93-4e37-bf8f-0b13e5624de3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-22T20:57:14.258152Z",
     "iopub.status.busy": "2024-10-22T20:57:14.257945Z",
     "iopub.status.idle": "2024-10-22T20:57:14.261468Z",
     "shell.execute_reply": "2024-10-22T20:57:14.260959Z",
     "shell.execute_reply.started": "2024-10-22T20:57:14.258136Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This code is running on computer vanille with user pierre\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import socket\n",
    "username = os.environ[\"USER\"]\n",
    "hostname = socket.gethostname()\n",
    "print(f\"This code is running on computer {hostname} with user {username}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ee82873-6a19-4919-96f1-3ccf9d5451aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-22T20:57:14.262438Z",
     "iopub.status.busy": "2024-10-22T20:57:14.261940Z",
     "iopub.status.idle": "2024-10-22T20:57:16.785013Z",
     "shell.execute_reply": "2024-10-22T20:57:16.784443Z",
     "shell.execute_reply.started": "2024-10-22T20:57:14.262422Z"
    }
   },
   "outputs": [],
   "source": [
    "if hostname.startswith(\"lk\"):\n",
    "    # University computers\n",
    "    model = SentenceTransformer(\n",
    "        model_name,\n",
    "        cache_folder=f\"/sratch/{username}/huggingface/hub\"\n",
    "    )\n",
    "else:\n",
    "    # Personal computers\n",
    "    model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea0273e-c6b0-4a20-a2f7-79b1ca1a2ab2",
   "metadata": {},
   "source": [
    "This is the kind of output you could expect to get while downloading the model:\n",
    "\n",
    "```\n",
    "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]\n",
    "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]\n",
    "README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]\n",
    "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]\n",
    "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]\n",
    "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]\n",
    "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]\n",
    "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]\n",
    "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]\n",
    "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]\n",
    "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e40fda6-e8ab-4105-b91a-4a4e2b129e3f",
   "metadata": {},
   "source": [
    "### Basic example\n",
    "\n",
    "Here is a couple of sentences to play with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40c242db-f361-4e74-853c-1b485fe03a57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-22T20:57:16.785782Z",
     "iopub.status.busy": "2024-10-22T20:57:16.785598Z",
     "iopub.status.idle": "2024-10-22T20:57:16.789178Z",
     "shell.execute_reply": "2024-10-22T20:57:16.788397Z",
     "shell.execute_reply.started": "2024-10-22T20:57:16.785764Z"
    }
   },
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"DNA carries genetic information in cells.\",\n",
    "    \"Proteins are made up of chains of amino acids.\",\n",
    "    \"DNA encodes the sequence of residues.\",\n",
    "    \"RNA is a type of nucleic acid.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8b0484-868a-40a0-8f7d-a6721bf88bdc",
   "metadata": {},
   "source": [
    "We get the embeddings for each sentence. Each embedding is a vector of 768 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5df89075-4557-49b3-99b4-45ac66de1bb8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-22T20:57:16.792360Z",
     "iopub.status.busy": "2024-10-22T20:57:16.792019Z",
     "iopub.status.idle": "2024-10-22T20:57:19.942661Z",
     "shell.execute_reply": "2024-10-22T20:57:19.942226Z",
     "shell.execute_reply.started": "2024-10-22T20:57:16.792330Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 768)\n",
      "[[-0.01080427  0.02138491 -0.03320902 ... -0.04414995 -0.02035814\n",
      "  -0.0343384 ]\n",
      " [ 0.05468469 -0.02634791  0.01001398 ... -0.07231469 -0.00555746\n",
      "  -0.0263908 ]\n",
      " [ 0.01480863 -0.00215497 -0.02918642 ...  0.00138177  0.01167384\n",
      "  -0.05353284]\n",
      " [ 0.06408021 -0.03017923  0.0217301  ... -0.02100319 -0.05333204\n",
      "  -0.05483887]]\n"
     ]
    }
   ],
   "source": [
    "embeddings = model.encode(sentences)\n",
    "print(embeddings.shape)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76aeb464-60a7-4708-8ec4-e981c7c80d80",
   "metadata": {},
   "source": [
    "Get similarity between all embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d99dbbe6-1515-490c-84a2-c66de443fe49",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-22T20:57:19.943345Z",
     "iopub.status.busy": "2024-10-22T20:57:19.943136Z",
     "iopub.status.idle": "2024-10-22T20:57:20.117242Z",
     "shell.execute_reply": "2024-10-22T20:57:20.115162Z",
     "shell.execute_reply.started": "2024-10-22T20:57:19.943324Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.3202, 0.6056, 0.4671],\n",
      "        [0.3202, 1.0000, 0.4517, 0.4007],\n",
      "        [0.6056, 0.4517, 1.0000, 0.4412],\n",
      "        [0.4671, 0.4007, 0.4412, 1.0000]])\n"
     ]
    }
   ],
   "source": [
    "similarities = model.similarity(embeddings, embeddings)\n",
    "print(similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16acb06b-57d0-4c95-8b3c-f5f06b7a02d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-22T14:31:47.126921Z",
     "iopub.status.busy": "2024-10-22T14:31:47.126459Z",
     "iopub.status.idle": "2024-10-22T14:31:47.131221Z",
     "shell.execute_reply": "2024-10-22T14:31:47.130565Z",
     "shell.execute_reply.started": "2024-10-22T14:31:47.126892Z"
    }
   },
   "source": [
    "We obtain a 4 x 4 square matrix. The diagonal is made of 1 because a sentence is identical to itself.\n",
    "\n",
    "Remark: This code also works to get similarities based on the cosin distance\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "similarities = np.inner(embeddings, embeddings)\n",
    "```\n",
    "\n",
    "We now display the most similar sentence for a given sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d21f3e1-31dd-4edb-84c2-0a70c5d3a97b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-22T20:57:20.120448Z",
     "iopub.status.busy": "2024-10-22T20:57:20.119589Z",
     "iopub.status.idle": "2024-10-22T20:57:20.144972Z",
     "shell.execute_reply": "2024-10-22T20:57:20.142605Z",
     "shell.execute_reply.started": "2024-10-22T20:57:20.120363Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence    : DNA carries genetic information in cells.\n",
      "Most similar sentence: DNA encodes the sequence of residues.\n",
      "\n",
      "Original sentence    : Proteins are made up of chains of amino acids.\n",
      "Most similar sentence: DNA encodes the sequence of residues.\n",
      "\n",
      "Original sentence    : DNA encodes the sequence of residues.\n",
      "Most similar sentence: DNA carries genetic information in cells.\n",
      "\n",
      "Original sentence    : RNA is a type of nucleic acid.\n",
      "Most similar sentence: DNA carries genetic information in cells.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for idx, sentence in enumerate(sentences):\n",
    "    # Discard similarity for the sentence itself.\n",
    "    # Score of 1 is remplaced by -1.\n",
    "    similarities[idx][idx] = -1\n",
    "    # Find index of the most similar sentence.\n",
    "    most_similar_idx = np.argmax(similarities[idx])\n",
    "    print(f\"Original sentence    : {sentence}\")\n",
    "    print(f\"Most similar sentence: {sentences[most_similar_idx]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19398170-95ab-412b-9fea-0b39dfab1213",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-22T15:04:19.020335Z",
     "iopub.status.busy": "2024-10-22T15:04:19.019019Z",
     "iopub.status.idle": "2024-10-22T15:04:19.027184Z",
     "shell.execute_reply": "2024-10-22T15:04:19.026374Z",
     "shell.execute_reply.started": "2024-10-22T15:04:19.020296Z"
    }
   },
   "source": [
    "What do you think of these results? Do you agree with the most similar sentences?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382df048-32fd-4aea-89ec-a73452e277dc",
   "metadata": {},
   "source": [
    "### Try by yourself\n",
    "\n",
    "Use different sentences and compare similarities between same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f26acff-00fe-4768-8371-28d058138687",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583673e0-0166-4c32-8e1a-97ffd327b89c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687b5f76-b438-48ce-8688-5fe1bfc4f036",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f249a7c-72ed-4cb4-9706-be75769b5569",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c26f53c-615c-42f7-9ac1-16469e6d9876",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7dce5ed8-0171-4727-a23a-ca0141c37474",
   "metadata": {},
   "source": [
    "### Other models\n",
    "\n",
    "The `all-mpnet-base-v2` model takes a maximum of 384 tokens as input.\n",
    "\n",
    "Larger embedding models are openly available, such as [Alibaba-NLP/gte-Qwen2-1.5B-instruct](https://huggingface.co/Alibaba-NLP/gte-Qwen2-1.5B-instruct):\n",
    "- 1.78B -- billions -- parameters (about 7 GB of data model to download)\n",
    "- embedding vector with 1,536 dimensions\n",
    "- max input tokens: 32k\n",
    "\n",
    "**If you want to use this more powerful model, be aware it will take some time to download onto your machine.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a259fc-c84d-429d-9b4e-4bc7e9fefae8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-22T15:41:47.749942Z",
     "iopub.status.busy": "2024-10-22T15:41:47.749212Z",
     "iopub.status.idle": "2024-10-22T15:43:01.334228Z",
     "shell.execute_reply": "2024-10-22T15:43:01.333584Z",
     "shell.execute_reply.started": "2024-10-22T15:41:47.749883Z"
    }
   },
   "source": [
    "For comparison, here is a list of commercial embedding models provided by [OpenAI](https://openai.com/api/pricing/):\n",
    "\n",
    "\n",
    "| Model                    | Description                                                                       | Max token | Output Dimension | Price ($US / 1M tokens) |\n",
    "| ------------------------ | --------------------------------------------------------------------------------- | --------- | ---------------- | ---------------------- |\n",
    "| `text-embedding-3-large` | Most capable embedding model for both english and non-english tasks               | 8191      | 3,072            | 0.13                   |\n",
    "| `text-embedding-3-small` | Increased performance over 2nd generation ada embedding model                     | 8191      | 1,536            | 0.02                   |\n",
    "| `text-embedding-ada-002` | Most capable 2nd generation embedding model, replacing 16 first generation models | 8191      | 1,536            | 0.10                   |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403dda5b-0935-4459-83a1-e68e00ed3616",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
