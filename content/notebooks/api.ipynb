{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce6faf5a-c6b9-46d4-ae6a-af75bb13940b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-22T19:12:31.801910Z",
     "iopub.status.busy": "2024-10-22T19:12:31.801643Z",
     "iopub.status.idle": "2024-10-22T19:12:31.809340Z",
     "shell.execute_reply": "2024-10-22T19:12:31.808256Z",
     "shell.execute_reply.started": "2024-10-22T19:12:31.801886Z"
    }
   },
   "source": [
    "# Playing with LLM APIs\n",
    "\n",
    "In this notebook, we will use large language models (LLMs) through an API. Most LLM providers offer an API to use their LLMs programmatically (for instance, with Python).\n",
    "\n",
    "[Groq](https://groq.com/) freely provides (as of October 2025) access to open LLMs, like Llama from Meta or Mixtral from Mistral AI ([list of models](https://console.groq.com/docs/models)).\n",
    "\n",
    "## Get credentials and API key\n",
    "\n",
    "- Create a free account on Groq: <https://console.groq.com/login>\n",
    "\n",
    "  You can use your GitHub or Google account for authenication.\n",
    "- Then go on [Groq's API keys](https://console.groq.com/keys) page\n",
    "- Click the black button  \"Create API Key\"\n",
    "- Chose a name for your key.\n",
    "- Copy your API key and paste it somewhere. For security reason, your API key won't be shown again. Your API key should look like: `gsk_W1Jz47u5ieJs28D8m...`\n",
    "\n",
    "## Setup\n",
    "\n",
    "Please check you have configured your environement properly with uv (see [setup](../setup.md))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8495392-22f6-4f85-919a-a851dd0822b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ignore this cell.\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda7e69a-a7f1-4b49-814f-72da5f3b0c28",
   "metadata": {},
   "source": [
    "## Use Groq's API\n",
    "\n",
    "Declare your API key with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4669842d-a75a-45d4-b725-500e5f175b88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "if \"GROQ_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GROQ_API_KEY\"] = \"YOUR-API-KEY-HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4ba0bd-faa7-448c-bc82-44982264463b",
   "metadata": {},
   "source": [
    "Replace `YOUR-API-KEY-HERE` with your real Groq API key.\n",
    "\n",
    "Then, you define a helper function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad175210-65c7-4297-a04d-0f107095ac6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "\n",
    "client = Groq(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    ")\n",
    "\n",
    "def chat(prompt=None, model=None):\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[ {\"role\": \"user\", \"content\": prompt} ],\n",
    "        model=model,\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbb00f6-3062-4ffc-a9fe-b8d93d85b9a0",
   "metadata": {},
   "source": [
    "Now you can send your first prompt to a Llama LLM model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c39b7131-fa7a-4010-8ed8-2068e3732b90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Bioinformatics is a field that combines biology and computers to understand living things. It's like being a detective for genes and cells. Scientists use computers to analyze huge amounts of data about DNA, proteins, and other biological things to figure out how they work, how they're related, and how they can be used to help people. It's a cool way to use technology to learn more about life and improve our health!\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\n",
    "    prompt=\"Explain in a short text what is bioinformatics to a middle school student\",\n",
    "    model=\"llama-3.3-70b-versatile\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da16e9e-74eb-4d5e-b34b-8efb412320ab",
   "metadata": {},
   "source": [
    "Look at [available LLM models in Groq](https://console.groq.com/docs/models) and try with two different models. \n",
    "\n",
    "```{warning}\n",
    "Be sure to select a LLM that works with chat. For instance, `whisper-large-v3-turbo` is not a chat model.\n",
    "```\n",
    "\n",
    "Here is another example with `openai/gpt-oss-20b`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0b5c09c-2689-42fc-a14a-36c388a21568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**What is bioinformatics?**  \\nThink of biology (the study of living things) as a giant library full of books, but instead of written pages, the books are made of DNA, proteins, and other molecules. Bioinformatics is the science that uses computers to read, organize, and analyze all that information—just like a super‑fast librarian that can find patterns, solve puzzles, and predict how a plant or a virus will behave. By crunching data with algorithms, scientists can discover new medicines, learn why certain genes cause disease, and even design better crops. In short, bioinformatics is biology + computer science, turning complex data into useful knowledge.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\n",
    "    prompt=\"Explain in a short text what is bioinformatics to a middle school student\",\n",
    "    model=\"openai/gpt-oss-20b\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36eee39d-ab5d-49cd-89e6-5d7b63d13718",
   "metadata": {},
   "source": [
    "## Are LLMs good at math?\n",
    "\n",
    "> Spoiler: no!\n",
    "\n",
    "In the following, we will assess the performance of a LLM in simple math calculations.\n",
    "\n",
    "### Find the correct prompt\n",
    "\n",
    "Design a prompt that forces the LLM to output only the result of the product of two integers. Test your prompt with several time with different integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f6c4d81-1899-469d-86c3-6a4d33538000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It seems like you forgot to include a prompt. What would you like to talk about or ask? I can provide information, answer questions, or engage in conversation on a wide range of topics.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\n",
    "    prompt=\"YOUR-CLEVER-PROMPT-HERE\",\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0778ea66-cab8-4fbf-b133-6e9e2a79c397",
   "metadata": {},
   "source": [
    "## LLM vs Python\n",
    "\n",
    "In the following code, we ask several times a LLM the result of the product of two integers and compare the output to the correct answer provided by Python.\n",
    "\n",
    "Take some time to understand how the code works.\n",
    "\n",
    "Run the code, play with it and make some improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce8aeda1-9893-491b-9dca-4bab52ff0ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "597 x 452 = 269844 | 270244 Wrong!\n",
      "215 x 228 = 49020 | 48920 Wrong!\n",
      "617 x 426 = 262842 | 263022 Wrong!\n",
      "596 x 279 = 166284 | 166524 Wrong!\n",
      "812 x 175 = 142100 | 142300 Wrong!\n",
      "638 x 953 = 608014 | 608134 Wrong!\n",
      "762 x 217 = 165354 | 165654 Wrong!\n",
      "552 x 308 = 170016 | 170016 OK\n",
      "644 x 546 = 351624 | 351704 Wrong!\n",
      "730 x 852 = 621960 | 621360 Wrong!\n",
      "614 x 983 = 603562 | 604502 Wrong!\n",
      "828 x 804 = 665712 | 664512 Wrong!\n",
      "653 x 193 = 126029 | 126049 Wrong!\n",
      "510 x 499 = 254490 | 254490 OK\n",
      "277 x 146 = 40442 | 40342 Wrong!\n",
      "841 x 405 = 340605 | 340605 OK\n",
      "229 x 198 = 45342 | 45362 Wrong!\n",
      "439 x 511 = 224329 | 224069 Wrong!\n",
      "582 x 268 = 155976 | 156216 Wrong!\n",
      "789 x 178 = 140442 | 140502 Wrong!\n",
      "Success rate: 15%\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "\n",
    "correct_answers = 0\n",
    "total_attempts = 20\n",
    "\n",
    "for _ in range(total_attempts):\n",
    "    number_1 = randint(100, 1_000)\n",
    "    number_2 = randint(100, 1_000)\n",
    "    llm_result = chat(\n",
    "        prompt=f\"What is the result of {number_1} times {number_2}. Only reply with the result\",\n",
    "        model=\"llama-3.3-70b-versatile\"\n",
    "    )\n",
    "    # Remove extra characters from LLM output.\n",
    "    llm_result = llm_result.replace(\",\", \"\").replace(\".\", \"\")\n",
    "    # Get Python result.\n",
    "    python_result = number_1 * number_2\n",
    "    print(f\"{number_1} x {number_2} = {python_result} | {llm_result} \", end=\"\")\n",
    "    # Check result.\n",
    "    if llm_result == str(python_result):\n",
    "        correct_answers += 1\n",
    "        print(f\"OK\")\n",
    "    else:\n",
    "        print(f\"Wrong!\")\n",
    "\n",
    "print(f\"Success rate: {correct_answers/total_attempts:.0%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b329543-70f8-4f3a-92c6-4e421d490698",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
