{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce6faf5a-c6b9-46d4-ae6a-af75bb13940b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-22T19:12:31.801910Z",
     "iopub.status.busy": "2024-10-22T19:12:31.801643Z",
     "iopub.status.idle": "2024-10-22T19:12:31.809340Z",
     "shell.execute_reply": "2024-10-22T19:12:31.808256Z",
     "shell.execute_reply.started": "2024-10-22T19:12:31.801886Z"
    }
   },
   "source": [
    "# Playing with LLM APIs\n",
    "\n",
    "In this notebook, we will use large language models (LLMs) through an API. Most LLM providers offer an API to use their LLMs programmatically (for instance, with Python).\n",
    "\n",
    "[Groq](https://groq.com/) provides access for free (as of October 2025) to open LLMs, like Llama from Meta or GPT OSS from OpenAI ([list of models](https://console.groq.com/docs/models)).\n",
    "\n",
    "## Get credentials and API key\n",
    "\n",
    "- Create a free account on Groq: <https://console.groq.com/login>\n",
    "\n",
    "  You can use your GitHub or Google account for authentication.\n",
    "- Then go on [Groq's API keys](https://console.groq.com/keys) page\n",
    "- Click the button \"+ Create API Key\"\n",
    "- Chose a name for your key.\n",
    "- Copy your API key and paste it somewhere. For security reason, your API key won't be shown again. Your API key should look like: `gsk_W1Jz47u5ieJs28D8m...`\n",
    "\n",
    "## Setup\n",
    "\n",
    "Please check you have configured your environement properly with uv (see [setup](../setup.md))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8495392-22f6-4f85-919a-a851dd0822b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ignore this cell.\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda7e69a-a7f1-4b49-814f-72da5f3b0c28",
   "metadata": {},
   "source": [
    "## Use Groq's API\n",
    "\n",
    "First, declare your API key with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4669842d-a75a-45d4-b725-500e5f175b88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "if \"GROQ_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GROQ_API_KEY\"] = \"YOUR-API-KEY-HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4ba0bd-faa7-448c-bc82-44982264463b",
   "metadata": {},
   "source": [
    "Replace `YOUR-API-KEY-HERE` with your real Groq API key.\n",
    "\n",
    "Then, define a helper function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad175210-65c7-4297-a04d-0f107095ac6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "\n",
    "client = Groq(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    ")\n",
    "\n",
    "def chat(prompt=None, model=None):\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[ {\"role\": \"user\", \"content\": prompt} ],\n",
    "        model=model,\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbb00f6-3062-4ffc-a9fe-b8d93d85b9a0",
   "metadata": {},
   "source": [
    "Now you can send your first prompt to a Llama LLM model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c39b7131-fa7a-4010-8ed8-2068e3732b90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Bioinformatics is a field that combines biology and computer science. It's like being a detective for living things. Bioinformaticians use computers to analyze and understand data about genes, proteins, and other biological things. They look for patterns and clues in the data to help us learn more about how living things work, how diseases happen, and how we can stay healthy. It's a really cool way to use computers to help us understand the world of biology and make new discoveries.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\n",
    "    prompt=\"Explain in a short text what is bioinformatics to a middle school student\",\n",
    "    model=\"llama-3.3-70b-versatile\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da16e9e-74eb-4d5e-b34b-8efb412320ab",
   "metadata": {},
   "source": [
    "Look at [available LLM models in Groq](https://console.groq.com/docs/models) and try with a different model.\n",
    "\n",
    "```{warning}\n",
    "Be sure to select a LLM that works with chat. For instance, `whisper-large-v3-turbo` is not a chat model.\n",
    "```\n",
    "\n",
    "Here is another example with `openai/gpt-oss-20b`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0b5c09c-2689-42fc-a14a-36c388a21568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**Bioinformatics** is the science of using computers to help scientists understand biology.  \\nJust like a detective uses clues to solve a mystery, bioinformaticians use software and algorithms to read and analyze the huge amount of information hidden in DNA, proteins, and cells.  \\nBy comparing many DNA sequences, they can find genes that make people healthy or sick, discover how plants grow, or design new medicines.  \\nThink of it as a giant digital laboratory where the data are the clues and the computer is the superâ€‘smart helper.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\n",
    "    prompt=\"Explain in a short text what is bioinformatics to a middle school student\",\n",
    "    model=\"openai/gpt-oss-20b\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36eee39d-ab5d-49cd-89e6-5d7b63d13718",
   "metadata": {},
   "source": [
    "## Are LLMs good at math?\n",
    "\n",
    "> Spoiler: no!\n",
    "\n",
    "In the following, we will assess the performance of a LLM in simple math calculations.\n",
    "\n",
    "### Find the correct prompt\n",
    "\n",
    "Design a prompt that forces the LLM to output only the result of the product of two integers. Test your prompt several time with different integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f6c4d81-1899-469d-86c3-6a4d33538000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You didn\\'t give me a prompt, so I\\'ll have to come up with something. \\n\\nLet\\'s play a game of \"Story Chain.\" I\\'ll start telling a story, and then stop at a cliffhanger. Your task is to continue the story in your own words, and then I\\'ll respond with the next part of the story, and so on.\\n\\nHere\\'s my first paragraph:\\n\\nIn a world where time was currency, the rich lived forever and the poor were left with nothing but the ticking of their clocks. The city of Chronos was the epicenter of this strange economy, where people traded years of their lives for material possessions and experiences. Amidst the hustle and bustle of the city, a young woman named Aria lived a life of quiet desperation. She had only a few hours left on her clock, and she was determined to make the most of it.\\n\\nNow it\\'s your turn! What happens next to Aria?'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\n",
    "    prompt=\"YOUR-CLEVER-PROMPT-HERE\",\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0778ea66-cab8-4fbf-b133-6e9e2a79c397",
   "metadata": {},
   "source": [
    "## LLM vs Python\n",
    "\n",
    "In the following code, we will ask a LLM the result of the product of two integers and compare the output to the correct answer provided by Python.\n",
    "\n",
    "Take some time to understand how the code works.\n",
    "\n",
    "Run the code, play with it and make some improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce8aeda1-9893-491b-9dca-4bab52ff0ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "695 x 452 = 314140 | 313940 Wrong!\n",
      "907 x 712 = 645784 | 646084 Wrong!\n",
      "697 x 809 = 563873 | 564793 Wrong!\n",
      "126 x 383 = 48258 | 48318 Wrong!\n",
      "291 x 869 = 252879 | 252879 OK\n",
      "105 x 809 = 84945 | 84845 Wrong!\n",
      "777 x 582 = 452214 | 451734 Wrong!\n",
      "738 x 602 = 444276 | 444876 Wrong!\n",
      "402 x 733 = 294666 | 294726 Wrong!\n",
      "113 x 175 = 19775 | 19775 OK\n",
      "284 x 691 = 196244 | 196124 Wrong!\n",
      "764 x 585 = 446940 | 447060 Wrong!\n",
      "798 x 122 = 97356 | 97316 Wrong!\n",
      "489 x 695 = 339855 | 339855 OK\n",
      "482 x 307 = 147974 | 148054 Wrong!\n",
      "327 x 768 = 251136 | 250416 Wrong!\n",
      "541 x 935 = 505835 | 505615 Wrong!\n",
      "711 x 141 = 100251 | 100011 Wrong!\n",
      "975 x 577 = 562575 | 562775 Wrong!\n",
      "472 x 334 = 157648 | 157648 OK\n",
      "Success rate: 20%\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "\n",
    "correct_answers = 0\n",
    "total_attempts = 20\n",
    "\n",
    "for _ in range(total_attempts):\n",
    "    number_1 = randint(100, 1_000)\n",
    "    number_2 = randint(100, 1_000)\n",
    "    llm_result = chat(\n",
    "        prompt=f\"What is the result of {number_1} times {number_2}. Only reply with the result\",\n",
    "        model=\"llama-3.3-70b-versatile\"\n",
    "    )\n",
    "    # Remove extra characters from LLM output.\n",
    "    llm_result = llm_result.replace(\",\", \"\").replace(\".\", \"\")\n",
    "    # Get Python result.\n",
    "    python_result = number_1 * number_2\n",
    "    print(f\"{number_1} x {number_2} = {python_result} | {llm_result} \", end=\"\")\n",
    "    # Check result.\n",
    "    if llm_result == str(python_result):\n",
    "        correct_answers += 1\n",
    "        print(f\"OK\")\n",
    "    else:\n",
    "        print(f\"Wrong!\")\n",
    "\n",
    "print(f\"Success rate: {correct_answers/total_attempts:.0%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b329543-70f8-4f3a-92c6-4e421d490698",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
