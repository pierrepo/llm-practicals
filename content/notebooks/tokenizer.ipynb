{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffcc70a2-016f-442c-a129-4a3d0c90551c",
   "metadata": {},
   "source": [
    "# Tokenizer with Python\n",
    "\n",
    "## Setup\n",
    "\n",
    "Please check you have configured your environement properly with uv (see [setup](../setup.md))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abd09c6-d42e-4787-adfe-fe8852e7d688",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "\n",
    "[tiktoken](https://github.com/openai/tiktoken) is an open-source Python library developped by OpenAI to tokenize text. This library works fully locally and does not require any internet connection.\n",
    "\n",
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df298dbe-6cb8-4853-8440-6a2009579f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39718f01-3867-439b-8fd3-887813422b6e",
   "metadata": {},
   "source": [
    "## First example\n",
    "\n",
    "Get tokens id from a simple sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7976f5a-2bef-488f-b235-18effce67ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9906, 1917]\n"
     ]
    }
   ],
   "source": [
    "tokens = enc.encode(\"Hello world\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d835ff-141a-42de-a976-59c61d8ee611",
   "metadata": {},
   "source": [
    "Visualize tokens by separating them with `|`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c096433f-0213-4407-adc8-6411d408c78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello| world\n"
     ]
    }
   ],
   "source": [
    "print(\"|\".join([ enc.decode([tok]) for tok in tokens]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880f388c-1828-4265-931f-2a0184738e0f",
   "metadata": {},
   "source": [
    "The first token is `Hello` and the second is `  world` (with a space before `world`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcf7f08-e9b1-4a3b-abbc-5eb48b951219",
   "metadata": {},
   "source": [
    "## Hello bioinformatics\n",
    "\n",
    "Let's try with another sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7abb2d44-abfe-46cb-bc5f-6f93163549fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9906, 17332, 98588]\n",
      "Hello| bio|informatics\n"
     ]
    }
   ],
   "source": [
    "tokens = enc.encode(\"Hello bioinformatics\")\n",
    "print(tokens)\n",
    "print(\"|\".join([ enc.decode([tok]) for tok in tokens]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bae090a-b87d-429b-85c6-6e7b95730f89",
   "metadata": {},
   "source": [
    "We have this time 3 tokens.\n",
    "\n",
    "Here is the same sentence in a different language:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00cea7b0-6fce-4152-883e-2d851c538b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17691, 332, 1208, 17332, 258, 2293, 2428]\n",
      "Sal|ut| la| bio|in|format|ique\n"
     ]
    }
   ],
   "source": [
    "tokens = enc.encode(\"Salut la bioinformatique\")\n",
    "print(tokens)\n",
    "print(\"|\".join([ enc.decode([tok]) for tok in tokens]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f3242e-2c45-4853-855a-b48d2bbf49af",
   "metadata": {},
   "source": [
    "The English word `bioinformatics` is expressed in 2 tokens whereas its French equivalent (`bioinformatique`) is made of 4 tokens.\n",
    "\n",
    "Tokenizers are optimized for the English language. Equivalent sentences in other languages usually takes more tokens. This is an important difference considering that costs to use LLM APIs are usually per (million) tokens.\n",
    "\n",
    "### Explore by yourself\n",
    "\n",
    "Compare tokenization of other sentences or words in different languages (English, French, Italian, Russian, Chinese...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bfafc1-407f-436b-a2bd-ade0dc5a711b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023f8029-847f-4613-aecb-7bbc65d0f184",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
