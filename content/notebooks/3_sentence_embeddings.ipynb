{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcbc1f82",
   "metadata": {},
   "source": [
    "# Sentence embeddings with Python\n",
    "\n",
    "In this section, we will explore more advanced embedding models that are based on the transformers neural network architecture. They also use tokens, semantic units smaller than words, that carries more semantic information than words. Theses models are also able to compute embedding for an entire piece of texte (like a sentence).\n",
    "\n",
    "## Setup\n",
    "\n",
    "Please check you have configured your environment properly with uv (see [setup](../setup.md))\n",
    "\n",
    "## Load required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "537ac36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pierre/workp/teaching/BI_M2_LLM/llm-practicals.git/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from fastembed import TextEmbedding\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be3b7ac",
   "metadata": {},
   "source": [
    "## Load the model\n",
    "\n",
    "This model is a reduced version of the model [`nomic-embed-text-v-1.5`](https://huggingface.co/nomic-ai/nomic-embed-text-v1.5). It is a 137-million parameter model that is trained on a large corpus of text to generate embeddings for sentences. Generated embeddings are 768-dimensional vectors that capture the semantic meaning of the input text.\n",
    "\n",
    "Loading the model can take a few minutes, so be patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8aa2335",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_687068/8704592.py:1: UserWarning: The model 'nomic-ai/nomic-embed-text-v1.5-Q' has been updated on HuggingFace. Please review the latest documentation on HF and release notes to ensure compatibility with your workflow. \n",
      "  model = TextEmbedding(\"nomic-ai/nomic-embed-text-v1.5-Q\")\n"
     ]
    }
   ],
   "source": [
    "model = TextEmbedding(\"nomic-ai/nomic-embed-text-v1.5-Q\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63579bac",
   "metadata": {},
   "source": [
    "## Explore sentence embeddings\n",
    "\n",
    "We will first compute the embedding vector for a sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8ec2653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The .embed() method returns a generator that we convert to a list\n",
    "# and then take the first element to get the embedding vector.\n",
    "vector = list(model.embed(\"Hello world!\"))[0]  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbc1be9",
   "metadata": {},
   "source": [
    "Display the embedding vector and its size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34b1ffa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ten first elements of the vector:\n",
      "[ 0.06642091  0.20711488 -4.30757713 -0.06292436 -0.65036206  1.85833189\n",
      " -0.11560041 -0.24147129 -0.5290895  -1.38232353]\n",
      "\n",
      "Vector size: 768\n"
     ]
    }
   ],
   "source": [
    "print(\"Ten first elements of the vector:\")\n",
    "print(vector[:10])\n",
    "print(f\"\\nVector size: {len(vector)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf17dd4",
   "metadata": {},
   "source": [
    "With this model, embeddings are 768-dimensional vectors.\n",
    "\n",
    "We can also compute the embedding for a list of sentences. The model will return a list of embeddings, one for each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ebf9e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Five first elements of the embedding vectors for each sentence:\n",
      "[[ 0.40734354 -0.12720139 -4.44438326 -0.22155395  0.56882895]\n",
      " [-0.13132822  0.18084796 -4.59579621  0.13161762 -0.6519796 ]\n",
      " [ 1.35002802  1.58990231 -3.40088714  0.38871316  0.80267872]\n",
      " [ 0.27677379  1.24250409 -4.05348274  0.25858318  0.99779707]\n",
      " [ 1.11604745  2.28325894 -2.88656797 -0.21893531  0.68955154]\n",
      " [ 1.77145601  2.12199622 -3.45457127 -0.72396081  1.12455912]]\n",
      "Shape of embedding vectors: (6, 768)\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"The weather is lovely today.\",\n",
    "    \"It's very sunny outside!\",\n",
    "    \"Are you watching the soccer game on TV?\",\n",
    "    \"I love playing football.\",\n",
    "    \"Are you studying nanoporous materials?\",\n",
    "    \"Physics is the science of matter and energy.\"\n",
    "]\n",
    "embeddings = np.array(list(model.embed(sentences)))\n",
    "print(\"\\nFive first elements of the embedding vectors for each sentence:\")\n",
    "print(embeddings[:, :5])\n",
    "print(f\"Shape of embedding vectors: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e045d1",
   "metadata": {},
   "source": [
    "In this example, we have 6 vectors with 768 dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d351c493",
   "metadata": {},
   "source": [
    "## Compare sentences\n",
    "\n",
    "We can compare the embeddings of two sentences by computing the cosine similarity between corresponding vectors.\n",
    "To speed up the computation, we will first normalize the embeddings.\n",
    "\n",
    "Nomalization and cosine similarity can be computed with the `scikit-learn` library or with the `numpy` library.\n",
    "\n",
    "### Method 1 with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fc170a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.7519107  0.45709374 0.48242015 0.35106745 0.45631191]\n",
      " [0.7519107  1.         0.53255499 0.49246394 0.34660804 0.43519417]\n",
      " [0.45709374 0.53255499 1.         0.5587763  0.43619317 0.42865404]\n",
      " [0.48242015 0.49246394 0.5587763  1.         0.34444231 0.39961487]\n",
      " [0.35106745 0.34660804 0.43619317 0.34444231 1.         0.54134965]\n",
      " [0.45631191 0.43519417 0.42865404 0.39961487 0.54134965 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Normalize embeddings\n",
    "embeddings_normalized = normalize(embeddings, axis=1)\n",
    "# Compute the cosine similarity between embeddings\n",
    "similarity_matrix = cosine_similarity(embeddings_normalized, embeddings_normalized)\n",
    "print(similarity_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b92f7cc",
   "metadata": {},
   "source": [
    "### Method 2 with Numpy\n",
    "\n",
    "Cosine similarity between vectors X and Y is the normalized dot product of X and Y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9f391e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.7519107  0.45709374 0.48242015 0.35106745 0.45631191]\n",
      " [0.7519107  1.         0.53255499 0.49246394 0.34660804 0.43519417]\n",
      " [0.45709374 0.53255499 1.         0.5587763  0.43619317 0.42865404]\n",
      " [0.48242015 0.49246394 0.5587763  1.         0.34444231 0.39961487]\n",
      " [0.35106745 0.34660804 0.43619317 0.34444231 1.         0.54134965]\n",
      " [0.45631191 0.43519417 0.42865404 0.39961487 0.54134965 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Normalize embeddings\n",
    "norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "embeddings_normalized = embeddings / norms\n",
    "# Compute cosine similarity matrix\n",
    "similarity_matrix = np.dot(embeddings_normalized, embeddings_normalized.T)\n",
    "print(similarity_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98d21ce",
   "metadata": {},
   "source": [
    "### Find most similar sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f908732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target sentence      : The weather is lovely today.\n",
      "Most similar sentence: It's very sunny outside!\n",
      "Cosine similarity    : 0.752\n",
      "----------------------------------------------------------------------\n",
      "Target sentence      : It's very sunny outside!\n",
      "Most similar sentence: The weather is lovely today.\n",
      "Cosine similarity    : 0.752\n",
      "----------------------------------------------------------------------\n",
      "Target sentence      : Are you watching the soccer game on TV?\n",
      "Most similar sentence: I love playing football.\n",
      "Cosine similarity    : 0.559\n",
      "----------------------------------------------------------------------\n",
      "Target sentence      : I love playing football.\n",
      "Most similar sentence: Are you watching the soccer game on TV?\n",
      "Cosine similarity    : 0.559\n",
      "----------------------------------------------------------------------\n",
      "Target sentence      : Are you studying nanoporous materials?\n",
      "Most similar sentence: Physics is the science of matter and energy.\n",
      "Cosine similarity    : 0.541\n",
      "----------------------------------------------------------------------\n",
      "Target sentence      : Physics is the science of matter and energy.\n",
      "Most similar sentence: Are you studying nanoporous materials?\n",
      "Cosine similarity    : 0.541\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Set the diagonal to -inf to exclude self-similarity.\n",
    "np.fill_diagonal(similarity_matrix, -np.inf)\n",
    "# Find the most similar sentence for each sentence.\n",
    "for i, sentence in enumerate(sentences):\n",
    "    row = similarity_matrix[i]\n",
    "    most_sim_idx = np.argmax(row)\n",
    "    print(f\"Target sentence      : {sentence}\")\n",
    "    print(f\"Most similar sentence: {sentences[most_sim_idx]}\")\n",
    "    print(f\"Cosine similarity    : {row[most_sim_idx]:.3f}\")\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17becba9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
