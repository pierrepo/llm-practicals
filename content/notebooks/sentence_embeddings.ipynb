{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcbc1f82",
   "metadata": {},
   "source": [
    "# Sentence embeddings\n",
    "\n",
    "In this section, we will explore more advanced embedding models that are based on the transformers neural network architecture. It also uses tokens, semantic units smaller than words, that carries more semantic information than words. Theses models are also able to compute embedding for an entire piece of texte (like a sentence).\n",
    "\n",
    "## Load required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "537ac36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastembed import TextEmbedding\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be3b7ac",
   "metadata": {},
   "source": [
    "## Load the model\n",
    "\n",
    "This model is a reduced version of the model [`nomic-embed-text-v-1.5`](https://huggingface.co/nomic-ai/nomic-embed-text-v1.5). It is a 137-million parameter model that is trained on a large corpus of text to generate embeddings for sentences. Generated embeddings are 768-dimensional vectors that capture the semantic meaning of the input text.\n",
    "\n",
    "Loading the model can take a few minutes, so be patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8aa2335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f501a65c0904dcfb849e9e53e280279",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = TextEmbedding(\"nomic-ai/nomic-embed-text-v1.5-Q\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63579bac",
   "metadata": {},
   "source": [
    "## Explore sentence embeddings\n",
    "\n",
    "We will first compute the embedding vector for sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8ec2653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The .embed() method returns a generator that we convert to a list\n",
    "# and then take the first element to get the vector.\n",
    "vector = list(model.embed(\"Hello world!\"))[0]  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbc1be9",
   "metadata": {},
   "source": [
    "Display the embedding vector and its size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34b1ffa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ten first elements of the vector:\n",
      "[ 0.06642091  0.20711488 -4.307577   -0.06292436 -0.6503621   1.8583319\n",
      " -0.11560041 -0.24147129 -0.5290895  -1.3823235 ]\n",
      "\n",
      "Vector size: 768\n"
     ]
    }
   ],
   "source": [
    "print(\"Ten first elements of the vector:\")\n",
    "print(vector[:10])\n",
    "print(f\"\\nVector size: {len(vector)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf17dd4",
   "metadata": {},
   "source": [
    "With this model, embeddings are 768-dimensional vectors.\n",
    "\n",
    "We can also compute the embeddings for a list of sentences. The model will return a list of embeddings, one for each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ebf9e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ten first elements of the embedding vectors for each sentence:\n",
      "[[ 0.41381216 -0.15882386 -4.4720335  -0.3561031   0.43502694  1.8090299\n",
      "   0.8556498  -0.48111734  0.32931688 -1.5628498 ]\n",
      " [-0.00912805  0.14920007 -4.606499   -0.16274986 -0.8524226   1.7413867\n",
      "   0.4728594   0.55459136  0.4093597  -1.1776258 ]\n",
      " [ 1.3647739   1.65053    -3.583437    0.36573407  0.54171497  0.06346153\n",
      "  -0.5936647  -0.33975348 -0.28960556 -0.58449554]\n",
      " [ 0.35631263  1.2773244  -3.928822    0.5598846   0.8928453   0.3641485\n",
      "  -0.6450936  -0.48812354  0.05629476 -1.0712796 ]\n",
      " [ 0.9096239   2.103455   -2.8272188  -0.3196854   0.72942066 -0.20781904\n",
      "   0.5402453   0.34872225  0.22590417  0.16909127]]\n",
      "Shape of embedding vectors: (5, 768)\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"The weather is lovely today.\",\n",
    "    \"It's very sunny outside!\",\n",
    "    \"Are you watching the soccer game on TV?\",\n",
    "    \"I love playing football.\",\n",
    "    \"Are you studying nanoporous materials?\",\n",
    "]\n",
    "embeddings = np.array(list(model.embed(sentences)))\n",
    "print(\"\\nTen first elements of the embedding vectors for each sentence:\")\n",
    "print(embeddings[:, :10])\n",
    "print(f\"Shape of embedding vectors: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e045d1",
   "metadata": {},
   "source": [
    "In this example, we have 5 vectors of 768 dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d351c493",
   "metadata": {},
   "source": [
    "## Compare sentences\n",
    "\n",
    "We can compare the embeddings of two sentences by computing the cosine similarity between corresponding vectors.\n",
    "To speed up the computation, we will first normalize the embeddings.\n",
    "\n",
    "Nomalization and cosine similarity can be computed with the `scikit-learn` library or with the `numpy` library.\n",
    "\n",
    "### Method 1 with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fc170a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.74607956 0.4681632  0.48687425 0.36103436]\n",
      " [0.74607956 1.         0.5475793  0.48899338 0.356462  ]\n",
      " [0.4681632  0.5475793  0.9999999  0.59073675 0.43514606]\n",
      " [0.48687425 0.48899338 0.59073675 1.         0.35732406]\n",
      " [0.36103436 0.356462   0.43514606 0.35732406 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Normalize embeddings\n",
    "embeddings_normalized = normalize(embeddings, axis=1)\n",
    "# Compute the cosine similarity between embeddings\n",
    "similarity_matrix = cosine_similarity(embeddings_normalized, embeddings_normalized)\n",
    "print(similarity_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b92f7cc",
   "metadata": {},
   "source": [
    "### Method 2 with Numpy\n",
    "\n",
    "Cosine similarity between vectors X and Y is the normalized dot product of X and Y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9f391e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99999994 0.7460795  0.4681632  0.48687416 0.3610344 ]\n",
      " [0.7460795  0.9999999  0.5475792  0.48899332 0.35646197]\n",
      " [0.4681632  0.5475792  1.         0.59073675 0.43514612]\n",
      " [0.48687416 0.48899332 0.59073675 0.99999994 0.35732403]\n",
      " [0.3610344  0.35646197 0.43514612 0.35732403 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Normalize embeddings\n",
    "norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "embeddings_normalized = embeddings / norms\n",
    "# Compute cosine similarity matrix\n",
    "similarity_matrix = np.dot(embeddings_normalized, embeddings_normalized.T)\n",
    "print(similarity_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98d21ce",
   "metadata": {},
   "source": [
    "### Find most similar sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f908732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target sentence      : The weather is lovely today.\n",
      "Most similar sentence: It's very sunny outside!\n",
      "Cosine similarity    : 0.746\n",
      "----------------------------------------------------------------------\n",
      "Target sentence      : It's very sunny outside!\n",
      "Most similar sentence: The weather is lovely today.\n",
      "Cosine similarity    : 0.746\n",
      "----------------------------------------------------------------------\n",
      "Target sentence      : Are you watching the soccer game on TV?\n",
      "Most similar sentence: I love playing football.\n",
      "Cosine similarity    : 0.591\n",
      "----------------------------------------------------------------------\n",
      "Target sentence      : I love playing football.\n",
      "Most similar sentence: Are you watching the soccer game on TV?\n",
      "Cosine similarity    : 0.591\n",
      "----------------------------------------------------------------------\n",
      "Target sentence      : Are you studying nanoporous materials?\n",
      "Most similar sentence: Are you watching the soccer game on TV?\n",
      "Cosine similarity    : 0.435\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Set the diagonal to -inf to exclude self-similarity.\n",
    "np.fill_diagonal(similarity_matrix, -np.inf)\n",
    "# Find the most similar sentence for each sentence.\n",
    "for i, sentence in enumerate(sentences):\n",
    "    row = similarity_matrix[i]\n",
    "    most_sim_idx = np.argmax(row)\n",
    "    print(f\"Target sentence      : {sentence}\")\n",
    "    print(f\"Most similar sentence: {sentences[most_sim_idx]}\")\n",
    "    print(f\"Cosine similarity    : {row[most_sim_idx]:.3f}\")\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17becba9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
